---
title: "Coursera Regression Modelling Project"
author: "Christopher Jones"
date: "December 18, 2017"
output:
  pdf_document: default
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
mean_diff <- NA
true_mean_diff <- NA
if(file.exists('mean_diff.rds')){mean_diff <- readRDS('mean_diff.rds')}
if(file.exists('true_mean_diff.rds')){true_mean_diff <- readRDS('true_mean_diff.rds')}
```

<!-- <center> <h2>Motor Trend Data Analysis:<br/>The Effect Of Transmission Type On Fuel Economy</h2> </center> --> 
\Large
\begin{center}Motor Trend Data Analysis:\\The Effect Of Transmission Type On Fuel Economy \end{center}
\normalsize

##{.tabset}


### Executive Summary

This report examines the impact on gas mileage attributable to car transmission type (automatic vs manual), controlling for a variety of other variables. Our data will be R's built-in `mtcars` dataset, and we will use regression modelling techniques in the analysis.

Despite at first glance there appearing to be a large (`r mean_diff`) increase in mileage from automatic to manual, taking account of the correlations among other variables shows this influence to be somewhat less. What remains after accounting for the primary variables is a more modest (`r true_mean_diff`) increase going from automatic to manual.


### Data Loading & Exploatory Analysis

First we load libaries we'll use, along with the data to be analyzed:

```{r libraries_data, message=FALSE}
library(ggplot2)
library(leaps)
library(RColorBrewer)
library(corrplot)
library(scales)

data(mtcars)
head(mtcars)
```

Next we perform some light data processing for easy later use.

```{r data_processing}
# Light preprocessing

# create factors
mtcars_fac <- mtcars
mtcars_fac$cyl <- as.factor(mtcars_fac$cyl)
mtcars_fac$vs <- as.factor(mtcars_fac$vs)
mtcars_fac$am <- factor(mtcars_fac$am)
levels(mtcars_fac$am) <- c("auto", "man")
mtcars_fac$gear <- factor(mtcars_fac$gear)
mtcars_fac$carb <- factor(mtcars_fac$carb)

# save object for later (2nd pass) use in the summary
mean_diff <- percent((round(mean(mtcars[mtcars$am == 1,]$mpg), digits=1) - round(mean(mtcars[mtcars$am == 0,]$mpg), digits=1)) / round(mean(mtcars[mtcars$am == 0,]$mpg), digits=1))
saveRDS(mean_diff, "mean_diff.rds")
```

Now we take a quick look at our data with violin plots, to get an initial idea of the data and their distributions.

The first plot is mileage vs transmission type, the predictor and outcome we are interested in here. We see a clear difference in their means and in their overall distributions. The mileages for automatics are concentrated near the group's mean of $\mu_A$ = `r round(mean(mtcars[mtcars$am == 0,]$mpg), digits=1)` mpg, while those for manuals are much more uniformly distributed along the category's range, with a mean of $\mu_M$ = `r round(mean(mtcars[mtcars$am == 1,]$mpg), digits=1)` mpg.

```{r exploratory_plot1, message=FALSE}
# Basic exploratory data analysis

# Plot MPG vs trans
g <- ggplot(mtcars_fac, aes(x=am, y=mpg, fill=am)) + 
  theme(legend.position="none"
        , panel.background = element_rect(fill='grey')
        , plot.background = element_rect(fill='darkseagreen')
        , plot.title = element_text(hjust = 0.5)
  ) +
  ggtitle('Mileage By Transmission Type') +
  labs(x="Transmission Type", y="MPG") +
  geom_violin(trim=TRUE) +
  scale_fill_brewer(palette="Blues") + 
  geom_boxplot(width=0.05) + 
  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = .5)

print(g)
```
<!--
But this 7 mpg difference in means of course fails to split out the influences & correlations among all of the other variables in the dataset. For example, we see a significant change when we break out the number of cylinders:

```{r exploratory_plot2, message=FALSE}
# Plot MPG vs (trans x cyls)
g1 <- ggplot(mtcars_fac, aes(x=am, y=mpg, fill=am)) + 
  theme(legend.position="none"
        , panel.background = element_rect(fill='grey')
        , plot.background = element_rect(fill='darkseagreen')
        , plot.title = element_text(hjust = 0.5)
        ) +
  ggtitle('Mileage By Transmission Type and # Of Cylinders') +
  labs(x="Transmission Type", y="MPG") +
  facet_wrap(~cyl, nrow=1) +
  geom_violin(trim=TRUE) +
  scale_fill_brewer(palette="Blues") + 
  geom_boxplot(width=0.05) + 
  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = .5)

print(g1)
```
-->


<!--For all we know, similar apparent changes could take place with any or all of the variables in the data - therefore a more princpled approach is needed in order to draw conclusions. In the next section we'll apply the best-subsets technique to determine the "proper" set of variables to use in our modelling.
-->

### Model Selection

We look at the variable correlation plot to begin model selection:

```{r variable_correlations}
# handy function for correlograms from
# http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram
cor.mtest <- function(mat, ...) {
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(mat[, i], mat[, j], ...)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
# p-values matrix of the correlations
p.mat <- cor.mtest(mtcars)

col <- colorRampPalette(brewer.pal(5, 'RdYlGn'))
corrplot(cor(mtcars)
         , method="color"
         , col=col(200)  
         , type="upper"
         , order="hclust" 
         , addCoef.col = "black"
         , tl.col="black"
         , tl.srt=45
         , p.mat = p.mat
         , sig.level = 0.01
         , insig = "blank" 
         , diag=FALSE 
)
```

With so much correlation, we wish to remove as many variables as possible to avoid model overfit (weak predictive ability) but no more (to avoid bias from underfit). To assist in determining which variables to remove, we use the best-subsets technique from the`leaps` package.

```{r best_subsets}
# best-subsets method of variable selection
best.subset <- regsubsets(mpg ~ ., mtcars, nvmax=10)
best.subset.summary <- summary(best.subset)
best.subset.summary$outmat
```

Because the purpose of this report is to determine the influence of transmission type (variable `am`), we reject the 2 most parsimonious models as they don't include the `am` variable.

To decide among the remaining model possibilities, we examine some of the key model performance indicators:

```{r model_kpis}
best.subset.by.adjr2 <- which.max(best.subset.summary$adjr2)
best.subset.by.cp <- which.min(best.subset.summary$cp)
best.subset.by.bic <- which.min(best.subset.summary$bic)

par(mfrow = c(2, 2), oma = c( 0, 0, 2, 0 ))
plot(best.subset$rss, xlab="Number of Variables", ylab="RSS", type="l")
plot(best.subset.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="l")
points(best.subset.by.adjr2, best.subset.summary$adjr2[best.subset.by.adjr2], col="red", cex =2, pch =20)
plot(best.subset.summary$cp, xlab="Number of Variables", ylab="CP", type="l")
points(best.subset.by.cp, best.subset.summary$cp[best.subset.by.cp], col="red", cex =2, pch =20)
plot(best.subset.summary$bic, xlab="Number of Variables", ylab="BIC", type="l")
points(best.subset.by.bic, best.subset.summary$bic[best.subset.by.bic], col="red", cex =2, pch =20)
title( "Regression Variable Selection KPIs", outer = TRUE )
```

So we see that while maximum variability explained is attained at the 5-variable model, there isn't much improvement beyond the 3-variable model. Also, desired minimums of CP (precision) and BIC (informativeness) are attained at the 3-variable model. Therefore we select the 3-variable model consisting of predictors `wt`, `qsec`, and `am`.


### Linear Regression & Diagnostics

Running the 3-variable model selected in the previous section, `mpg` ~ `wt` + `qsec` + `am`, gives us:

```{r best_model}
fit <- lm(formula = mpg ~ wt + qsec + am, data = mtcars)
summary(fit)

# save object for later (2nd pass) use in the summary
true_mean_diff <- percent(fit$coefficients["am"] / round(mean(mtcars[mtcars$am == 0,]$mpg), digits=1))
saveRDS(true_mean_diff, "true_mean_diff.rds")
```

Our model appears to explain about 83% of the variability in the data (close to the maximum). According to the model, when weight and quarter second time are held constant, going from automatic to manual transmission changes mileage by about 2.92 mpg.

Now we run some diagnostics to verify the goodness of our selected model.

```{r residual_plots}
par(mfrow = c(2, 2), oma = c( 0, 0, 2, 0 ))
plot(fit)
```

With this diagnostics plot, we observe the following:

1. On the residuals vs fitted plot there appears to be no particular pattern to the residuals.
2. The residual quantile-quantile plot falls approximately on the x=y line, supporting the condition that errors are normally distributed.
3. The scale-location plot is similar to the residuals plot - the points appear randomly spread.
4. There appear to a a couple of candidates for outliers in the data: the Chrysler Imperial and the Mercedes 230. Further investigation may determine these observations are better off excluded.

### Conclusions

We have seen in this report that a good linear model indicates an increase of approximately 2.92 mpg when switching from automatic to manual transmission (holding primary predictors constant). Other candidate models were considered, being rejected either as not being informative/parsimonious, or for not containing the predictor we are focussed on in this report. Robustness of the chosen model was verified via standard residual plots.