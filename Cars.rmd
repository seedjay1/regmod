---
title: "Coursera Regression Modelling Project"
author: "Christopher Jones"
date: "December 13, 2017"
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
mean_diff <- NA
if(file.exists('mean_diff.rds')){mean_diff <- readRDS('mean_diff.rds')}
```

<center> <h2>Motor Trend Data Analysis:<br/>The Effect Of Transmission Type On Mileage</h2> </center>

##{.tabset}

### Executive Summary

This report examines the impact on gas mileage attributable to car transmission type (automatic vs manual), controlling for a variety of other variables. Our data will be R's built-in `mtcars` dataset, and we will use regression modelling techniques in the analysis.

Despite at first glance there appearing to be a large `r mean_diff` increase in mileage from automatic to manual) influence of automatic vs manual transmission on gas mileage, taking account of the correlations of other variables lessens this impact significantly. What remains after accounting for the other variables is a modest 7% difference in gas mileage attributable to transmission type.


### Data Loading & Exploatory Analysis

First we load libaries we'll use, along with the data to be analyzed:


```{r libraries_data}
library(ggplot2)
library(leaps)
library(corrplot)
library(scales)

data(mtcars)
head(mtcars)
```

Next we perform some light data processing for easy later use.

```{r data_processing}
# Light preprocessing

# create factors
mtcars_fac <- mtcars
mtcars_fac$cyl <- as.factor(mtcars_fac$cyl)
mtcars_fac$vs <- as.factor(mtcars_fac$vs)
mtcars_fac$am <- factor(mtcars_fac$am)
levels(mtcars_fac$am) <- c("auto", "man")
#mtcars_fac$trans <- as.factor(ifelse(mtcars_fac$am == "auto", 1, 0)) # auto=1, manual = 0
mtcars_fac$gear <- factor(mtcars_fac$gear)
mtcars_fac$carb <- factor(mtcars_fac$carb)

# save object for later (2nd pass) use in the summary
mean_diff <- percent((round(mean(mtcars[mtcars$am == 1,]$mpg), digits=1) - round(mean(mtcars[mtcars$am == 0,]$mpg), digits=1)) / round(mean(mtcars[mtcars$am == 0,]$mpg), digits=1))
saveRDS(mean_diff, "mean_diff.rds")

```

Now we take a quick look at our data with violin plots, to get an initial idea of the data and their distributions.

The first plot is mileage vs transmission type, the predictor and outcome we are interested in here. We see a clear difference in their means and in their overall distributions. The mileages for automatics are concentrated near the group's mean of $\mu_A$ = `r round(mean(mtcars[mtcars$am == 0,]$mpg), digits=1)` mpg, while those for manuals are much more uniformly distributed along the category's range, with a mean of $\mu_M$ = `r round(mean(mtcars[mtcars$am == 1,]$mpg), digits=1)` mpg.

```{r exploratory_plot1}
# Basic exploratory data analysis

# Plot MPG vs trans
g <- ggplot(mtcars_fac, aes(x=am, y=mpg, fill=am)) + 
  theme(legend.position="none"
        , panel.background = element_rect(fill='grey')
        , plot.background = element_rect(fill='darkseagreen')
        , plot.title = element_text(hjust = 0.5)
  ) +
  ggtitle('Mileage By Transmission Type') +
  labs(x="Transmission Type", y="MPG") +
  geom_violin(trim=TRUE) +
  scale_fill_brewer(palette="Blues") + 
  geom_boxplot(width=0.05) + 
  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = .5)

print(g)
```

But this 7 mpg difference in means of course fails to split out the influences & correlations among all of the other variables in the dataset. For example, we see a significant change when we break out the number of cylinders:

```{r exploratory_plot2}
# Plot MPG vs (trans x cyls)
g1 <- ggplot(mtcars_fac, aes(x=am, y=mpg, fill=am)) + 
  theme(legend.position="none"
        , panel.background = element_rect(fill='grey')
        , plot.background = element_rect(fill='darkseagreen')
        , plot.title = element_text(hjust = 0.5)
        ) +
  ggtitle('Mileage By Transmission Type and # Of Cylinders') +
  labs(x="Transmission Type", y="MPG") +
  facet_wrap(~cyl, nrow=1) +
  geom_violin(trim=TRUE) +
  scale_fill_brewer(palette="Blues") + 
  geom_boxplot(width=0.05) + 
  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = .5)

print(g1)
```

For all we know, similar apparent changes could take place with any or all of the variables in the data - therefore a more princpled approach is need. In the next section we'll apply the best-subsets technique to determine the "proper" set of variables to use in our modelling.


### Regression Modelling

Examination of the correlation plot shows us that a large amount of correlation is present in the available variables:

```{r variable_correlations}
cor.mtest <- function(mat, ...) {
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(mat[, i], mat[, j], ...)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
# matrix of the p-value of the correlation
p.mat <- cor.mtest(mtcars)

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor(mtcars), method="color", col=col(200),  
         type="upper", order="hclust", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, #Text label color and rotation
         # Combine with significance
         #p.mat = p.mat, sig.level = 0.01, insig = "blank", 
         # hide correlation coefficient on the principal diagonal
         diag=FALSE 
)
```

With so much correlation, we wish to remove as many variables as possible to avoid model overfit (weak predictive ability) but no more (to avoid bias from underfit). To assist in determining which variables to remove, we use the best-subsets technique from the`leaps` package.

```{r best_subsets}
# all subsets method of variable selection
best.subset <- regsubsets(mpg ~ ., mtcars, nvmax=10)
best.subset.summary <- summary(best.subset)
best.subset.summary$outmat
```

Because the purpose of this report is to determine the influence of transmission type (variable "am"), we reject the 2 most parsimonious models because they don't include the am variable.

### Model Diagnostics

this is where we will analyze the model's goodness of fit


### Conclusions

this is where we will quantify our conclusions, but in English

and coefficient interpretation