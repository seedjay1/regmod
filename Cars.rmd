---
title: "Coursera Regression Modelling Project"
author: "Christopher Jones"
date: "December 18, 2017"
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
mean_diff <- NA
true_mean_diff <- NA
if(file.exists('mean_diff.rds')){mean_diff <- readRDS('mean_diff.rds')}
if(file.exists('true_mean_diff.rds')){true_mean_diff <- readRDS('true_mean_diff.rds')}
```

<center> <h2>Motor Trend Data Analysis:<br/>The Effect Of Transmission Type On Fuel Economy</h2> </center>

##{.tabset}


### Executive Summary

This report examines the impact on gas mileage attributable to car transmission type (automatic vs manual), controlling for a variety of other variables. Our data will be R's built-in `mtcars` dataset, and we will use regression modelling techniques in the analysis.

Despite at first glance there appearing to be a large (`r mean_diff`) increase in mileage from automatic to manual, taking account of the correlations among other variables shows this influence to be somewhat less. What remains after accounting for the other variables is a more modest (`r true_mean_diff`) increase going from automatic to manual.


### Data Loading & Exploatory Analysis

First we load libaries we'll use, along with the data to be analyzed:

```{r libraries_data}
library(ggplot2)
library(leaps)
library(corrplot)
library(scales)

data(mtcars)
head(mtcars)
```

Next we perform some light data processing for easy later use.

```{r data_processing}
# Light preprocessing

# create factors
mtcars_fac <- mtcars
mtcars_fac$cyl <- as.factor(mtcars_fac$cyl)
mtcars_fac$vs <- as.factor(mtcars_fac$vs)
mtcars_fac$am <- factor(mtcars_fac$am)
levels(mtcars_fac$am) <- c("auto", "man")
#mtcars_fac$trans <- as.factor(ifelse(mtcars_fac$am == "auto", 1, 0)) # auto=1, manual = 0
mtcars_fac$gear <- factor(mtcars_fac$gear)
mtcars_fac$carb <- factor(mtcars_fac$carb)

# save object for later (2nd pass) use in the summary
mean_diff <- percent((round(mean(mtcars[mtcars$am == 1,]$mpg), digits=1) - round(mean(mtcars[mtcars$am == 0,]$mpg), digits=1)) / round(mean(mtcars[mtcars$am == 0,]$mpg), digits=1))
saveRDS(mean_diff, "mean_diff.rds")
```

Now we take a quick look at our data with violin plots, to get an initial idea of the data and their distributions.

The first plot is mileage vs transmission type, the predictor and outcome we are interested in here. We see a clear difference in their means and in their overall distributions. The mileages for automatics are concentrated near the group's mean of $\mu_A$ = `r round(mean(mtcars[mtcars$am == 0,]$mpg), digits=1)` mpg, while those for manuals are much more uniformly distributed along the category's range, with a mean of $\mu_M$ = `r round(mean(mtcars[mtcars$am == 1,]$mpg), digits=1)` mpg.

```{r exploratory_plot1}
# Basic exploratory data analysis

# Plot MPG vs trans
g <- ggplot(mtcars_fac, aes(x=am, y=mpg, fill=am)) + 
  theme(legend.position="none"
        , panel.background = element_rect(fill='grey')
        , plot.background = element_rect(fill='darkseagreen')
        , plot.title = element_text(hjust = 0.5)
  ) +
  ggtitle('Mileage By Transmission Type') +
  labs(x="Transmission Type", y="MPG") +
  geom_violin(trim=TRUE) +
  scale_fill_brewer(palette="Blues") + 
  geom_boxplot(width=0.05) + 
  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = .5)

print(g)
```

But this 7 mpg difference in means of course fails to split out the influences & correlations among all of the other variables in the dataset. For example, we see a significant change when we break out the number of cylinders:

```{r exploratory_plot2}
# Plot MPG vs (trans x cyls)
g1 <- ggplot(mtcars_fac, aes(x=am, y=mpg, fill=am)) + 
  theme(legend.position="none"
        , panel.background = element_rect(fill='grey')
        , plot.background = element_rect(fill='darkseagreen')
        , plot.title = element_text(hjust = 0.5)
        ) +
  ggtitle('Mileage By Transmission Type and # Of Cylinders') +
  labs(x="Transmission Type", y="MPG") +
  facet_wrap(~cyl, nrow=1) +
  geom_violin(trim=TRUE) +
  scale_fill_brewer(palette="Blues") + 
  geom_boxplot(width=0.05) + 
  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = .5)

print(g1)
```

For all we know, similar apparent changes could take place with any or all of the variables in the data - therefore a more princpled approach is needed. In the next section we'll apply the best-subsets technique to determine the "proper" set of variables to use in our modelling.


### Model Selection

Examination of the correlation plot shows us that a large amount of correlation is present in the available variables:

```{r variable_correlations}
cor.mtest <- function(mat, ...) {
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(mat[, i], mat[, j], ...)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
# matrix of the p-value of the correlation
p.mat <- cor.mtest(mtcars)

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor(mtcars)
         , method="color"
         , col=col(200)  
         , type="upper"
         , order="hclust" 
         , addCoef.col = "black"
         , tl.col="black"
         , tl.srt=45
         , p.mat = p.mat
         , sig.level = 0.01
         , insig = "blank" 
         , diag=FALSE 
)
```

With so much correlation, we wish to remove as many variables as possible to avoid model overfit (weak predictive ability) but no more (to avoid bias from underfit). To assist in determining which variables to remove, we use the best-subsets technique from the`leaps` package.

```{r best_subsets}
# all subsets method of variable selection
best.subset <- regsubsets(mpg ~ ., mtcars, nvmax=10)
best.subset.summary <- summary(best.subset)
best.subset.summary$outmat
```

Because the purpose of this report is to determine the influence of transmission type (variable `am`), we reject the 2 most parsimonious models because they don't include the `am` variable.

For the remaining model possibilities, we examine some of the key model performance indicators:

```{r model_kpis}
best.subset.by.adjr2 <- which.max(best.subset.summary$adjr2)
best.subset.by.cp <- which.min(best.subset.summary$cp)
best.subset.by.bic <- which.min(best.subset.summary$bic)

par(mfrow = c(2, 2), oma = c( 0, 0, 2, 0 ))
plot(best.subset$rss, xlab="Number of Variables", ylab="RSS", type="l")
plot(best.subset.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="l")
points(best.subset.by.adjr2, best.subset.summary$adjr2[best.subset.by.adjr2], col="red", cex =2, pch =20)
plot(best.subset.summary$cp, xlab="Number of Variables", ylab="CP", type="l")
points(best.subset.by.cp, best.subset.summary$cp[best.subset.by.cp], col="red", cex =2, pch =20)
plot(best.subset.summary$bic, xlab="Number of Variables", ylab="BIC", type="l")
points(best.subset.by.bic, best.subset.summary$bic[best.subset.by.bic], col="red", cex =2, pch =20)
title( "Regression Variable Selection KPIs", outer = TRUE )
```

So we see that while maximum variability explained is attained at the 5 variable model, there isn't much improvement beyond the 3 variable model. Also, desired minimums of CP (precision) and BIC (informativeness) are attained at the 3 variable model. Therefore we select the 3 variable model consisting of predictors `wt`, `qsec`, and `am`.


### Regression & Model Diagnostics

```{r best_model}
fit <- lm(formula = mpg ~ wt + qsec + am + hp, data = mtcars)
summary(fit)

# save object for later (2nd pass) use in the summary
true_mean_diff <- percent(fit$coefficients["am"] / round(mean(mtcars[mtcars$am == 0,]$mpg), digits=1))
saveRDS(true_mean_diff, "true_mean_diff.rds")
```

Our model appears to explain about 84% of the variability in the data (close to the maximum). According to the model, when weight and quarter second time are held constant, going from automatic to manual transmission changes mileage by about 2.92 mpg.

Now we run some diagnostics to verify the goodness of our selected model.


### Conclusions

this is where we will quantify our conclusions, but in English

and coefficient interpretation